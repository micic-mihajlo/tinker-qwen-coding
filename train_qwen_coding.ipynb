{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Qwen3-30B-A3B for Coding with Tinker API\n",
    "\n",
    "This notebook fine-tunes Qwen3-30B-A3B on:\n",
    "1. **Phase 1**: rStar-Coder dataset (125K coding examples)\n",
    "2. **Phase 2**: SWE-bench tool-use conversations (229 examples)\n",
    "\n",
    "**Note**: The actual model training happens on Tinker's cloud servers. This notebook handles data preprocessing and API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tinker datasets transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your Tinker API key\n",
    "import os\n",
    "os.environ[\"TINKER_API_KEY\"] = \"tml-8zb1lan2FmC69nOsIE4jwd6MdTp9Oc2MCmL8IJfKkIk42U4CAGIFCzY6K8TRKjeXCAAAA\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import tinker\n",
    "from tinker import types\n",
    "from tinker.types.tensor_data import TensorData\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B\"\n",
    "LORA_RANK = 32\n",
    "\n",
    "# Phase 1: Coding\n",
    "PHASE1_LEARNING_RATE = 5e-5\n",
    "PHASE1_BATCH_SIZE = 128\n",
    "PHASE1_MAX_LENGTH = 2048\n",
    "PHASE1_MAX_SAMPLES = None  # Set to e.g. 10000 for faster testing\n",
    "\n",
    "# Phase 2: Tool Use\n",
    "PHASE2_LEARNING_RATE = 5e-5\n",
    "PHASE2_BATCH_SIZE = 32\n",
    "PHASE2_MAX_LENGTH = 8192\n",
    "PHASE2_NUM_EPOCHS = 3\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"/content/data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"LoRA Rank: {LORA_RANK}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Prepare rStar-Coder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversation_to_datum(conversation: Dict, tokenizer, max_length: int) -> Optional[types.Datum]:\n",
    "    \"\"\"Convert a conversation to a Tinker Datum.\"\"\"\n",
    "    messages = conversation.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    except Exception:\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg.get(\"role\", \"user\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "        text = \"\\n\".join(parts)\n",
    "\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(tokens) > max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    if len(tokens) < 2:\n",
    "        return None\n",
    "\n",
    "    # Create weights for assistant tokens\n",
    "    weights = [0.0] * (len(tokens) - 1)\n",
    "    text_so_far = \"\"\n",
    "    in_assistant = False\n",
    "    \n",
    "    for i, token in enumerate(tokens[:-1]):\n",
    "        decoded = tokenizer.decode([token])\n",
    "        text_so_far += decoded\n",
    "        if \"assistant\" in text_so_far[-20:].lower() and \"<|im_start|>\" in text_so_far[-30:]:\n",
    "            in_assistant = True\n",
    "        elif \"<|im_end|>\" in decoded:\n",
    "            in_assistant = False\n",
    "        elif \"<|im_start|>\" in decoded:\n",
    "            in_assistant = False\n",
    "        if in_assistant:\n",
    "            weights[i] = 1.0\n",
    "\n",
    "    if sum(weights) == 0:\n",
    "        for i in range(len(weights) // 4, len(weights)):\n",
    "            weights[i] = 1.0\n",
    "\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs={\n",
    "            \"target_tokens\": TensorData.from_numpy(np.array(target_tokens, dtype=np.int64)),\n",
    "            \"weights\": TensorData.from_numpy(np.array(weights, dtype=np.float32)),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for cached datums\n",
    "phase1_cache = DATA_DIR / \"phase1_datums.pkl\"\n",
    "\n",
    "if phase1_cache.exists():\n",
    "    print(\"Loading cached Phase 1 datums...\")\n",
    "    with open(phase1_cache, \"rb\") as f:\n",
    "        phase1_datums = pickle.load(f)\n",
    "    print(f\"Loaded {len(phase1_datums)} datums from cache\")\n",
    "else:\n",
    "    print(\"Loading rStar-Coder dataset...\")\n",
    "    dataset = load_dataset(\"microsoft/rStar-Coder\", \"seed_sft\", split=\"train\")\n",
    "    print(f\"Total examples: {len(dataset)}\")\n",
    "    \n",
    "    # Filter\n",
    "    if \"verified\" in dataset.column_names:\n",
    "        dataset = dataset.filter(lambda x: x.get(\"verified\", True))\n",
    "        print(f\"After verified filter: {len(dataset)}\")\n",
    "    if \"is_passed\" in dataset.column_names:\n",
    "        dataset = dataset.filter(lambda x: x.get(\"is_passed\", True))\n",
    "        print(f\"After is_passed filter: {len(dataset)}\")\n",
    "    \n",
    "    if PHASE1_MAX_SAMPLES:\n",
    "        dataset = dataset.select(range(min(PHASE1_MAX_SAMPLES, len(dataset))))\n",
    "        print(f\"Using {len(dataset)} samples\")\n",
    "    \n",
    "    # Convert to conversations\n",
    "    print(\"Converting to conversations...\")\n",
    "    conversations = []\n",
    "    for row in tqdm(dataset, desc=\"Converting\"):\n",
    "        question = row.get(\"question\", \"\")\n",
    "        response = row.get(\"response\", \"\")\n",
    "        code = row.get(\"code\", \"\")\n",
    "        \n",
    "        if response and code:\n",
    "            assistant_content = f\"{response}\\n\\n```python\\n{code}\\n```\"\n",
    "        elif code:\n",
    "            assistant_content = f\"```python\\n{code}\\n```\"\n",
    "        elif response:\n",
    "            assistant_content = response\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        conversations.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(conversations)} conversations\")\n",
    "    \n",
    "    # Tokenize with parallel processing\n",
    "    print(\"Tokenizing (this takes a few minutes)...\")\n",
    "    phase1_datums = []\n",
    "    \n",
    "    for conv in tqdm(conversations, desc=\"Tokenizing\"):\n",
    "        datum = conversation_to_datum(conv, tokenizer, PHASE1_MAX_LENGTH)\n",
    "        if datum:\n",
    "            phase1_datums.append(datum)\n",
    "    \n",
    "    print(f\"Created {len(phase1_datums)} datums\")\n",
    "    \n",
    "    # Cache for later\n",
    "    print(\"Caching datums...\")\n",
    "    with open(phase1_cache, \"wb\") as f:\n",
    "        pickle.dump(phase1_datums, f)\n",
    "    print(f\"Cached to {phase1_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_nll(logprobs: List, weights: List) -> float:\n",
    "    \"\"\"Compute mean negative log-likelihood.\"\"\"\n",
    "    total_loss = 0.0\n",
    "    total_weight = 0.0\n",
    "    for lp, w in zip(logprobs, weights):\n",
    "        if hasattr(lp, 'to_numpy'):\n",
    "            lp_arr = lp.to_numpy()\n",
    "        else:\n",
    "            lp_arr = np.array(lp)\n",
    "        if hasattr(w, 'to_numpy'):\n",
    "            w_arr = w.to_numpy()\n",
    "        else:\n",
    "            w_arr = np.array(w)\n",
    "        total_loss += float(np.sum(-lp_arr * w_arr))\n",
    "        total_weight += float(np.sum(w_arr))\n",
    "    return total_loss / total_weight if total_weight > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tinker client\n",
    "print(\"Creating Tinker training client...\")\n",
    "service_client = tinker.ServiceClient()\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=MODEL_NAME,\n",
    "    rank=LORA_RANK\n",
    ")\n",
    "print(\"Training client ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Training Loop\n",
    "n_batches = len(phase1_datums) // PHASE1_BATCH_SIZE\n",
    "print(f\"Phase 1: {n_batches} batches of {PHASE1_BATCH_SIZE}\")\n",
    "\n",
    "# Shuffle\n",
    "np.random.shuffle(phase1_datums)\n",
    "\n",
    "phase1_losses = []\n",
    "pbar = tqdm(range(n_batches), desc=\"Phase 1 Training\")\n",
    "\n",
    "for step in pbar:\n",
    "    # Get batch\n",
    "    batch_start = step * PHASE1_BATCH_SIZE\n",
    "    batch = phase1_datums[batch_start:batch_start + PHASE1_BATCH_SIZE]\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_mult = max(0.0, 1.0 - step / n_batches)\n",
    "    current_lr = PHASE1_LEARNING_RATE * lr_mult\n",
    "    \n",
    "    adam_params = types.AdamParams(\n",
    "        learning_rate=current_lr,\n",
    "        beta1=0.9,\n",
    "        beta2=0.95,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Forward-backward\n",
    "    fwd_bwd_future = training_client.forward_backward(batch, loss_fn=\"cross_entropy\")\n",
    "    optim_future = training_client.optim_step(adam_params)\n",
    "    \n",
    "    fwd_bwd_result = fwd_bwd_future.result()\n",
    "    optim_future.result()\n",
    "    \n",
    "    # Compute loss\n",
    "    train_logprobs = [x[\"logprobs\"] for x in fwd_bwd_result.loss_fn_outputs]\n",
    "    train_weights = [d.loss_fn_inputs[\"weights\"] for d in batch]\n",
    "    train_nll = compute_mean_nll(train_logprobs, train_weights)\n",
    "    phase1_losses.append(train_nll)\n",
    "    \n",
    "    pbar.set_postfix({\"NLL\": f\"{train_nll:.4f}\", \"LR\": f\"{current_lr:.2e}\"})\n",
    "    \n",
    "    # Save checkpoint every 100 steps\n",
    "    if step > 0 and step % 100 == 0:\n",
    "        save_result = training_client.save_state(name=f\"phase1-{step:06d}\").result()\n",
    "        print(f\"\\nCheckpoint saved: {save_result.path}\")\n",
    "\n",
    "print(\"\\nPhase 1 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Phase 1 checkpoint\n",
    "phase1_save = training_client.save_state(name=\"phase1-final\").result()\n",
    "phase1_path = phase1_save.path\n",
    "print(f\"Phase 1 final checkpoint: {phase1_path}\")\n",
    "\n",
    "# Plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(phase1_losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"NLL\")\n",
    "plt.title(\"Phase 1: Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Prepare SWE-bench Tool-Use Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for cached datums\n",
    "phase2_cache = DATA_DIR / \"phase2_datums.pkl\"\n",
    "\n",
    "if phase2_cache.exists():\n",
    "    print(\"Loading cached Phase 2 datums...\")\n",
    "    with open(phase2_cache, \"rb\") as f:\n",
    "        phase2_datums = pickle.load(f)\n",
    "    print(f\"Loaded {len(phase2_datums)} datums from cache\")\n",
    "else:\n",
    "    print(\"Loading SWE-bench tool-use dataset...\")\n",
    "    dataset = load_dataset(\n",
    "        \"AlexCuadron/SWE-Bench-Verified-O1-native-tool-calling-reasoning-high-results\",\n",
    "        split=\"test\"\n",
    "    )\n",
    "    print(f\"Total examples: {len(dataset)}\")\n",
    "    \n",
    "    # Filter for resolved\n",
    "    dataset = dataset.filter(lambda x: x.get(\"resolved\", False) == True)\n",
    "    print(f\"After resolved filter: {len(dataset)}\")\n",
    "    \n",
    "    # Parse conversations\n",
    "    print(\"Parsing conversations...\")\n",
    "    conversations = []\n",
    "    \n",
    "    for row in tqdm(dataset, desc=\"Parsing\"):\n",
    "        conv_str = row.get(\"full_conversation_jsonl\", \"\")\n",
    "        if not conv_str:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            turns = json.loads(conv_str)\n",
    "            messages = []\n",
    "            \n",
    "            for turn in turns:\n",
    "                if not isinstance(turn, dict):\n",
    "                    continue\n",
    "                \n",
    "                for msg in turn.get(\"messages\", []):\n",
    "                    content_parts = msg.get(\"content\", [])\n",
    "                    if isinstance(content_parts, str):\n",
    "                        content = content_parts\n",
    "                    elif isinstance(content_parts, list):\n",
    "                        text_parts = [p.get(\"text\", \"\") if isinstance(p, dict) and p.get(\"type\") == \"text\" else str(p) if isinstance(p, str) else \"\" for p in content_parts]\n",
    "                        content = \"\\n\".join(text_parts)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    if content.strip():\n",
    "                        role = msg.get(\"role\", \"assistant\")\n",
    "                        if role not in [\"user\", \"assistant\", \"system\"]:\n",
    "                            role = \"assistant\"\n",
    "                        messages.append({\"role\": role, \"content\": content.strip()})\n",
    "                \n",
    "                response = turn.get(\"response\", \"\")\n",
    "                if response and isinstance(response, str) and response.strip():\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": response.strip()})\n",
    "            \n",
    "            if messages:\n",
    "                # Merge consecutive same-role messages\n",
    "                cleaned = []\n",
    "                for msg in messages:\n",
    "                    if cleaned and cleaned[-1][\"role\"] == msg[\"role\"]:\n",
    "                        cleaned[-1][\"content\"] += \"\\n\\n\" + msg[\"content\"]\n",
    "                    else:\n",
    "                        cleaned.append(msg)\n",
    "                \n",
    "                # Add system message\n",
    "                system_msg = {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a helpful coding assistant working on issue {row.get('issue_name', 'unknown')} in project {row.get('project', 'unknown')}. Use tools to fix the issue.\"\n",
    "                }\n",
    "                conversations.append({\"messages\": [system_msg] + cleaned})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Created {len(conversations)} conversations\")\n",
    "    \n",
    "    # Tokenize\n",
    "    print(\"Tokenizing...\")\n",
    "    phase2_datums = []\n",
    "    for conv in tqdm(conversations, desc=\"Tokenizing\"):\n",
    "        datum = conversation_to_datum(conv, tokenizer, PHASE2_MAX_LENGTH)\n",
    "        if datum:\n",
    "            phase2_datums.append(datum)\n",
    "    \n",
    "    print(f\"Created {len(phase2_datums)} datums\")\n",
    "    \n",
    "    # Cache\n",
    "    with open(phase2_cache, \"wb\") as f:\n",
    "        pickle.dump(phase2_datums, f)\n",
    "    print(f\"Cached to {phase2_cache}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 uses the checkpoint from Phase 1\n",
    "# training_client already has Phase 1 weights loaded\n",
    "\n",
    "n_batches = max(1, len(phase2_datums) // PHASE2_BATCH_SIZE)\n",
    "total_steps = n_batches * PHASE2_NUM_EPOCHS\n",
    "print(f\"Phase 2: {n_batches} batches Ã— {PHASE2_NUM_EPOCHS} epochs = {total_steps} steps\")\n",
    "\n",
    "phase2_losses = []\n",
    "step = 0\n",
    "\n",
    "for epoch in range(PHASE2_NUM_EPOCHS):\n",
    "    np.random.shuffle(phase2_datums)\n",
    "    pbar = tqdm(range(n_batches), desc=f\"Phase 2 Epoch {epoch+1}/{PHASE2_NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch_idx in pbar:\n",
    "        batch_start = batch_idx * PHASE2_BATCH_SIZE\n",
    "        batch = phase2_datums[batch_start:batch_start + PHASE2_BATCH_SIZE]\n",
    "        \n",
    "        if not batch:\n",
    "            continue\n",
    "        \n",
    "        lr_mult = max(0.0, 1.0 - step / total_steps)\n",
    "        current_lr = PHASE2_LEARNING_RATE * lr_mult\n",
    "        \n",
    "        adam_params = types.AdamParams(\n",
    "            learning_rate=current_lr,\n",
    "            beta1=0.9,\n",
    "            beta2=0.95,\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        fwd_bwd_future = training_client.forward_backward(batch, loss_fn=\"cross_entropy\")\n",
    "        optim_future = training_client.optim_step(adam_params)\n",
    "        \n",
    "        fwd_bwd_result = fwd_bwd_future.result()\n",
    "        optim_future.result()\n",
    "        \n",
    "        train_logprobs = [x[\"logprobs\"] for x in fwd_bwd_result.loss_fn_outputs]\n",
    "        train_weights = [d.loss_fn_inputs[\"weights\"] for d in batch]\n",
    "        train_nll = compute_mean_nll(train_logprobs, train_weights)\n",
    "        phase2_losses.append(train_nll)\n",
    "        \n",
    "        pbar.set_postfix({\"NLL\": f\"{train_nll:.4f}\", \"LR\": f\"{current_lr:.2e}\"})\n",
    "        step += 1\n",
    "\n",
    "print(\"\\nPhase 2 complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_save = training_client.save_state(name=\"final\").result()\n",
    "sampler_save = training_client.save_weights_for_sampler(name=\"final-sampler\").result()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Final state path: {final_save.path}\")\n",
    "print(f\"Sampler path: {sampler_save.path}\")\n",
    "print(\"\\nUse the sampler path with Tinker's sampling API or OpenAI-compatible endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both phases\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].plot(phase1_losses)\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"NLL\")\n",
    "axes[0].set_title(\"Phase 1: Coding (rStar-Coder)\")\n",
    "\n",
    "axes[1].plot(phase2_losses)\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"NLL\")\n",
    "axes[1].set_title(\"Phase 2: Tool Use (SWE-bench)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling client\n",
    "sampling_client = service_client.create_sampling_client(model_path=sampler_save.path)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Write a Python function to find the longest palindromic substring in a string.\"\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "\n",
    "result = sampling_client.sample(\n",
    "    prompt=types.ModelInput.from_ints(tokens=prompt_tokens),\n",
    "    num_samples=1,\n",
    "    sampling_params=types.SamplingParams(max_tokens=512, temperature=0.7)\n",
    ").result()\n",
    "\n",
    "response = tokenizer.decode(result.sequences[0].tokens)\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(f\"\\nResponse:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
